{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f36d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "from preprocessing.filterHistoricalForecast_v2 import produce_filtered_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveData(dist):\n",
    "    \"\"\"\n",
    "    Returns date indexed dataframe of historical forecast data filtered within dist km of turbines\n",
    "    \n",
    "    Parameters:\n",
    "        dist (int): Distance in km from turbines to filter data\n",
    "    \n",
    "    Returns:\n",
    "        df (pd.DataFrame): DataFrame with date as index and sensor data as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = f'data/filtered_historicalForecasts/{dist}km_historicalForecast2024.csv'\n",
    "    print(f'Looking for file at {file_path}...', end=' ')\n",
    "    if os.path.exists(file_path):\n",
    "        print(f'FOUND!')\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        print('File does not exist, creating filtered dataset')\n",
    "        df = produce_filtered_dataset(\n",
    "            unfiltered_data_path = 'data/unfiltered_historicalForecast2024.csv',\n",
    "            turbine_data_path = 'data/uswtdb_v7_2_20241120.csv',\n",
    "            coordinate_column_path = 'data/coordinate_columns.csv',\n",
    "            output_folder = 'data/filtered_historicalForecasts',\n",
    "            max_distance_km = dist\n",
    "        )\n",
    "        \n",
    "    df['Date'] = df['Date'].apply(lambda x: x if \" \" in x else x + \" 00:00:00\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_hours(df):\n",
    "    \"\"\"\n",
    "    Identifies missing hourly timestamps in a DataFrame and adds rows with NaN values.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with datetime index or column\n",
    "        datetime_col (str): Name of datetime column (if not using index)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with complete hourly sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create complete hourly range for the year\n",
    "    start_date = df.index.min().floor('D')  # Start at beginning of first day\n",
    "    end_date = df.index.max().ceil('D')     # End at midnight of last day\n",
    "    full_range = pd.date_range(start=start_date, end=end_date, freq='h', inclusive='left')\n",
    "    \n",
    "    # Reindex to add missing hours\n",
    "    df_complete = df.reindex(full_range)\n",
    "    \n",
    "    return df_complete\n",
    "\n",
    "\n",
    "def getMagDf(dist=10):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the magnitude of wind speed for each sensor (forward filled for missing hours)\n",
    "    \n",
    "    Parameters:\n",
    "        dist (int): Distance in km from turbines to filter data\n",
    "        \n",
    "    Returns:\n",
    "        df_mag (pd.DataFrame): DataFrame with date as index and sensor magnitude as columns\n",
    "    \"\"\"\n",
    "\n",
    "    data = retrieveData(dist)\n",
    "\n",
    "    sensor_ids = sorted(set(col.split('_')[1] for col in data.columns if col.startswith('u80_')))\n",
    "\n",
    "    mag_columns = {}\n",
    "\n",
    "    for sensor_id in sensor_ids:\n",
    "        u_col = f'u80_{sensor_id}'\n",
    "        v_col = f'v80_{sensor_id}'\n",
    "        if u_col in data.columns and v_col in data.columns:\n",
    "            mag_columns[sensor_id] = np.sqrt(data[u_col]**2 + data[v_col]**2)\n",
    "\n",
    "    # Concatenate all columns at once\n",
    "    df_mag = pd.DataFrame(mag_columns, index=data.index)\n",
    "    df_mag = df_mag[sorted(df_mag.columns)]\n",
    "\n",
    "    df_mag = fill_missing_hours(df_mag).ffill()\n",
    "        \n",
    "    return df_mag\n",
    "\n",
    "\n",
    "def makeKernel(df):\n",
    "    \"\"\"\n",
    "    Creates a kernel covariance matrix for the given DataFrame using Gaussian Process Regression.\n",
    "        \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing sensor data over time with 1 column per sensor \n",
    "        \n",
    "    Returns:\n",
    "        K (np.ndarray): Covariance matrix of the kernel\n",
    "    \"\"\"\n",
    "    \n",
    "    vals = df.values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(vals)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X_scaled)\n",
    "\n",
    "    # Heuristic for distance\n",
    "    dists = pdist(X_scaled.T)  # pairwise distances\n",
    "    median_dist = np.median(dists)\n",
    "\n",
    "    kernel = RBF(length_scale=median_dist/2)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel)\n",
    "    K = gp.kernel(X_imputed, X_imputed)\n",
    "    return K\n",
    "\n",
    "def getCoordinateGrid(df):\n",
    "    \"\"\"\n",
    "    Returns ordered lat and lon dicts with coordinates\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing sensor data over time with 1 column per sensor \n",
    "        \n",
    "    Returns:\n",
    "        lat_dict (dict): Dictionary with sensor IDs as keys and latitude as values\n",
    "        lon_dict (dict): Dictionary with sensor IDs as keys and longitude as values\n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_ids = df.columns.astype(int)\n",
    "            \n",
    "    df_sensor_coords = pd.read_csv('data/coordinate_columns.csv')\n",
    "    \n",
    "    lat_dict = {sensor_id: df_sensor_coords[df_sensor_coords['sensor_id'] == sensor_id]['latitude'].iloc[0] for sensor_id in sensor_ids}\n",
    "    lon_dict = {sensor_id: df_sensor_coords[df_sensor_coords['sensor_id'] == sensor_id]['longitude'].iloc[0] for sensor_id in sensor_ids}\n",
    "\n",
    "    return lat_dict, lon_dict\n",
    "\n",
    "\n",
    "def mutual_info_gain(K_sub, sigma2):\n",
    "    \"\"\"\n",
    "    Computes the mutual information gain for a subset of the covariance matrix\n",
    "    Args:\n",
    "        K_sub: Subset of the covariance matrix\n",
    "        sigma2: Variance of observed values\n",
    "    Returns:\n",
    "        mutual_info: Mutual information gain\n",
    "    \"\"\"\n",
    "    n = K_sub.shape[0]\n",
    "    return 0.5 * np.linalg.slogdet(np.eye(n) + (1 / sigma2) * K_sub)[1]\n",
    "\n",
    "\n",
    "def greedy_select(K_full, sensor_id_to_index, k=10, sigma2=1e-5):\n",
    "    \"\"\"\n",
    "    Greedy Mutual Information Optimization to select points of maximum information\n",
    "\n",
    "    Args:\n",
    "        K_full: kernel matrix (N x N)\n",
    "        sensor_id_to_index: dict mapping sensor_id to kernel index (0-based)\n",
    "        k: number of sensors to select\n",
    "        sigma2: observation noise variance\n",
    "\n",
    "    Returns:\n",
    "        selected_ids: list of selected sensor IDs\n",
    "    \"\"\"\n",
    "    remaining_ids = list(sensor_id_to_index.keys())\n",
    "    selected_ids = []\n",
    "\n",
    "    for _ in range(k):\n",
    "        best_gain = -np.inf\n",
    "        best_sensor = None\n",
    "\n",
    "        for sensor_id in remaining_ids:\n",
    "            candidate_ids = selected_ids + [sensor_id]\n",
    "            candidate_indices = [sensor_id_to_index[sid] for sid in candidate_ids]\n",
    "\n",
    "            K_sub = K_full[np.ix_(candidate_indices, candidate_indices)]\n",
    "            gain = mutual_info_gain(K_sub, sigma2)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_sensor = sensor_id\n",
    "\n",
    "        selected_ids.append(best_sensor)\n",
    "        remaining_ids.remove(best_sensor)\n",
    "\n",
    "    return selected_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b381c3",
   "metadata": {},
   "source": [
    "### caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433fad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file at data/filtered_historicalForecasts/5km_historicalForecast2024.csv... FOUND!\n",
      "Selecting 1 of 10 sensors\n",
      "Selected sensor ID: 10077\n",
      "Selecting 2 of 10 sensors\n",
      "Selected sensor ID: 3149\n",
      "Selecting 3 of 10 sensors\n",
      "Selected sensor ID: 4500\n",
      "Selecting 4 of 10 sensors\n",
      "Selected sensor ID: 4797\n",
      "Selecting 5 of 10 sensors\n",
      "Selected sensor ID: 17339\n",
      "Selecting 6 of 10 sensors\n",
      "Selected sensor ID: 3025\n",
      "Selecting 7 of 10 sensors\n",
      "Selected sensor ID: 9000\n",
      "Selecting 8 of 10 sensors\n",
      "Selected sensor ID: 4497\n",
      "Selecting 9 of 10 sensors\n",
      "Selected sensor ID: 3151\n",
      "Selecting 10 of 10 sensors\n",
      "Selected sensor ID: 7532\n",
      "[10077, 3149, 4500, 4797, 17339, 3025, 9000, 4497, 3151, 7532]\n"
     ]
    }
   ],
   "source": [
    "df_mag = getMagDf(5)\n",
    "\n",
    "#square root to stabilize variance\n",
    "df_mag_sqrt = np.sqrt(df_mag)\n",
    "K = makeKernel(df_mag_sqrt)\n",
    "\n",
    "lat_dict, lon_dict = getCoordinateGrid(df_mag_sqrt)\n",
    "sensor_id_to_index = {sensor_id: idx for idx, sensor_id in enumerate(df_mag.columns.astype(int))}\n",
    "k_best_indices = greedy_select(K_full=K,sensor_id_to_index=sensor_id_to_index,k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
